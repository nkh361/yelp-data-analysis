{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Data-Driven Insights into Restaurant Markets Using Hybrid Clustering and Regression on Yelp Data\n",
    "\n",
    "This project presents a hybrid machine learning system that\n",
    "integrates both social computing and business analytics to provide\n",
    "data-driven insights for restaurant planning. By analyzing\n",
    "both restaurant characteristics and social signals, we demonstrate how social\n",
    "computing data can enhance business analytics.\n",
    "\n",
    "Primary questions of exploration:\n",
    "- Can we identify natural groupings of restaurant markets based on their characteristics?\n",
    "- Can we predict how well a restaurant may perform in a particular area based on it's features (category, price range, sentiment,\n",
    "- and cluster type)?\n",
    "- What cities exhibit similar restaurant market patterns?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd, numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "# os.system('pip install --upgrade ipywidgets')\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_restaurants(path='../data/Yelp JSON/yelp_dataset/yelp_academic_dataset_business.json'):\n",
    "    restaurants = []\n",
    "    with open(path) as json_file:\n",
    "        for line in tqdm(json_file, desc='Loading restaurants', colour='green'):\n",
    "            b = json.loads(line)\n",
    "            if b['categories'] and 'Restaurants' in b['categories'] and b['is_open'] == 1:\n",
    "                restaurants.append({\n",
    "                    'business_id': b['business_id'],\n",
    "                    'name': b['name'],\n",
    "                    'city': b['city'],\n",
    "                    'state': b['state'],\n",
    "                    'postal_code': b['postal_code'],\n",
    "                    'latitude': b['latitude'],\n",
    "                    'longitude': b['longitude'],\n",
    "                    'stars': b['stars'],\n",
    "                    'review_count': b['review_count'],\n",
    "                    'categories': b['categories'],\n",
    "                    'attributes': b['attributes'],\n",
    "                    'hours': b['hours']\n",
    "                })\n",
    "    return pd.DataFrame(restaurants)\n",
    "\n",
    "restaurants_df = load_restaurants()\n",
    "print(f\"Loaded {len(restaurants_df)} restaurants\")\n",
    "restaurants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restaurants_df['price_range'] = restaurants_df['attributes'].apply(\n",
    "#     lambda x: x.get('RestaurantsPriceRange2') if isinstance(x, dict) else None\n",
    "# ).astype('float').fillna(2).astype(int)\n",
    "#\n",
    "# attr_cols = ['RestaurantsTakeOut', 'RestaurantsDelivery', 'OutdoorSeating',\n",
    "#              'RestaurantsReservations', 'HasTV', 'Alcohol', 'WiFi', 'GoodForKids']\n",
    "#\n",
    "# for col in attr_cols:\n",
    "#     restaurants_df[col] = restaurants_df['attributes'].apply(\n",
    "#         lambda x: x.get(col) if isinstance(x, dict) else None\n",
    "#     )\n",
    "#     # convert the string values to boolean\n",
    "#     restaurants_df[col] = restaurants_df[col].map({'True': True, 'False': False, 'None': False, None: False}).fillna(False)\n",
    "\n",
    "# safer methods for lambda transformation\n",
    "def extract_price(attrs):\n",
    "    if not attrs or not isinstance(attrs, dict):\n",
    "        return None\n",
    "    val = attrs.get('RestaurantsPriceRange2')\n",
    "    if val in (None, 'None', ''):\n",
    "        return None\n",
    "    try:\n",
    "        return int(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def parse_bool(attrs, key, default=False):\n",
    "    if not attrs or not isinstance(attrs, dict):\n",
    "        return default\n",
    "    val = attrs.get(key)\n",
    "    if val in (True, 'True', \"u'True'\", \"'True'\", 'Yes'):\n",
    "        return True\n",
    "    if val in (False, 'False', \"u'False'\", \"'False'\", 'No', 'None', None):\n",
    "        return False\n",
    "    return default\n",
    "\n",
    "# apply them cleanly\n",
    "restaurants_df['price_range'] = restaurants_df['attributes'].apply(extract_price).fillna(2).astype(int)\n",
    "\n",
    "bool_attrs = [\n",
    "    'RestaurantsTakeOut', 'RestaurantsDelivery', 'OutdoorSeating',\n",
    "    'RestaurantsReservations', 'HasTV', 'WiFi', 'GoodForKids'\n",
    "]\n",
    "\n",
    "for attr in bool_attrs:\n",
    "    restaurants_df[attr] = restaurants_df['attributes'].apply(lambda x: parse_bool(x, attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkin_counts(checkin_path='../data/Yelp JSON/yelp_dataset/yelp_academic_dataset_checkin.json'):\n",
    "    counts = {}\n",
    "\n",
    "    with open(checkin_path, encoding='utf-8') as f:\n",
    "        for line in tqdm(f, total=131930, desc=\"Processing check-ins\"):\n",
    "            obj = json.loads(line)\n",
    "            bid = obj['business_id']\n",
    "            # count the number of checkins\n",
    "            count = len(obj['date'].split(',')) if obj['date'].strip() else 0\n",
    "            counts[bid] = count\n",
    "\n",
    "    return pd.DataFrame(list(counts.items()), columns=['business_id', 'checkin_count'])\n",
    "\n",
    "checkin_df = load_checkin_counts()\n",
    "\n",
    "# previous fix: drop any existing checkin_count column before merging\n",
    "if 'checkin_count' in restaurants_df.columns:\n",
    "    restaurants_df = restaurants_df.drop(columns=['checkin_count'])\n",
    "if 'checkin_count_x' in restaurants_df.columns:\n",
    "    restaurants_df = restaurants_df.drop(columns=['checkin_count_x'])\n",
    "if 'checkin_count_y' in restaurants_df.columns:\n",
    "    restaurants_df = restaurants_df.drop(columns=['checkin_count_y'])\n",
    "\n",
    "restaurants_df = restaurants_df.merge(checkin_df, on='business_id', how='left')\n",
    "\n",
    "# handle missing with 0 and make integer\n",
    "restaurants_df['checkin_count'] = restaurants_df['checkin_count'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"{restaurants_df['checkin_count'].sum():,} total check-ins across all restaurants\")\n",
    "print(f\"Restaurants with over 100 check-ins: {(restaurants_df['checkin_count'] >= 100).sum():,}\")\n",
    "restaurants_df[['name', 'city', 'checkin_count']].sort_values('checkin_count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_keywords = [\n",
    "    'Mexican', 'Italian', 'Chinese', 'Japanese', 'Indian', 'Thai', 'French',\n",
    "    'American', 'Mediterranean', 'Vietnamese', 'Korean', 'Greek', 'Cajun',\n",
    "    'Spanish', 'Middle Eastern', 'Caribbean', 'German', 'Irish', 'Pizza'\n",
    "]\n",
    "\n",
    "def get_main_cuisine(categories):\n",
    "    if not categories:\n",
    "        return 'Other'\n",
    "    cats = [c.strip() for c in categories.split(',')]\n",
    "    for cuisine in cuisine_keywords:\n",
    "        if cuisine in cats:\n",
    "            return cuisine\n",
    "    return 'Other'\n",
    "\n",
    "restaurants_df['cuisine'] = restaurants_df['categories'].apply(get_main_cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_market_density(df, n_clusters=1200, new_col='market_cluster_density'):\n",
    "    print(f\"creating {n_clusters} micro markets using KMeans on location\")\n",
    "\n",
    "    coords = df[['longitude', 'latitude']].values\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    df['market_cluster'] = kmeans.fit_predict(coords)\n",
    "\n",
    "    cluster_sizes = df['market_cluster'].value_counts()\n",
    "    df[new_col] = df['market_cluster'].map(cluster_sizes)\n",
    "\n",
    "    df[new_col] = df[new_col] - 1\n",
    "\n",
    "    print(f\"Most crowded micro market has {df[new_col].max() + 1} restaurants\")\n",
    "    return df\n",
    "\n",
    "restaurants_df = kmeans_market_density(restaurants_df, new_col='cluster_density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Previous considerations included groupings in 2km ranges. With KMeans, we are able to discover 1200 natural micro markets\n",
    "across North America. The ```cluster_density``` will be a strong feature in later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# success_score feature, score = percentile rank within each city\n",
    "restaurants_df['log_reviews'] = np.log1p(restaurants_df['review_count'])\n",
    "\n",
    "restaurants_df['success_score'] = restaurants_df.groupby('city')['log_reviews']\\\n",
    "    .transform(lambda x: x.rank(pct=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Updated (static) plots in final cell output, there were too many issues displaying Plotly plots in HTML exports and github.\n",
    "\"\"\"\n",
    "# fig1 = px.scatter_mapbox(\n",
    "#     restaurants_df.sample(min(8000, len(restaurants_df))),\n",
    "#     lat=\"latitude\", lon=\"longitude\",\n",
    "#     color=\"stars\", size=\"review_count\",\n",
    "#     hover_name=\"name\", hover_data=[\"city\", \"cuisine\"],\n",
    "#     title=\"Yelp Restaurants: Size = Popularity, Color = Rating\",\n",
    "#     zoom=3, height=600\n",
    "# )\n",
    "# fig1.update_layout(mapbox_style=\"carto-positron\")\n",
    "# fig1.show()\n",
    "#\n",
    "# fig2 = px.bar(restaurants_df['city'].value_counts().head(15),\n",
    "#               title=\"Top 15 cities by restaurant count\",\n",
    "#               labels={'value': 'Number of Restaurants', 'index': 'City'},\n",
    "#               color=restaurants_df['city'].value_counts().head(15).values,\n",
    "#               color_continuous_scale=\"Blues\")\n",
    "# fig2.update_xaxes(tickangle=45)\n",
    "# fig2.show()\n",
    "#\n",
    "# fig3 = px.histogram(restaurants_df, x='success_score', nbins=50,\n",
    "#                     title=\"Success score distribution (by percentile within city)\",\n",
    "#                     color_discrete_sequence=['#636EFA'])\n",
    "# fig3.update_layout(bargap=0.1)\n",
    "# fig3.show()\n",
    "#\n",
    "# fig4 = px.scatter(restaurants_df.sample(10000),\n",
    "#                   x='cluster_density', y='success_score',\n",
    "#                   color='price_range', size='review_count',\n",
    "#                   hover_data=['name', 'city', 'cuisine'],\n",
    "#                   title=\"Market Hotspot Density vs Success Score\",\n",
    "#                   labels={'cluster_density': 'Competitors in Natural Micro-Market',\n",
    "#                           'success_score': 'Success Score (percentile)'},\n",
    "#                   color_continuous_scale=\"Portland\")\n",
    "# fig4.show()\n",
    "\n",
    "underserved = restaurants_df[restaurants_df['success_score'] < 30]\n",
    "print(f\"Underserved locations (bottom 30% in their city): {len(underserved):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to central csv\n",
    "restaurants_df.to_csv('yelp_restaurants_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing pipeline\n",
    "feature_cols = ['latitude', 'longitude', 'price_range', 'checkin_count',\n",
    "                'cluster_density', 'cuisine', 'city']\n",
    "\n",
    "X = restaurants_df[feature_cols]\n",
    "y = restaurants_df['success_score']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), ['latitude', 'longitude', 'price_range',\n",
    "                                'checkin_count', 'cluster_density']),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
    "     ['cuisine', 'city'])\n",
    "], remainder='drop')\n",
    "\n",
    "X_final = preprocessor.fit_transform(X)\n",
    "print(f\"preprocessed matrix shape: {X_final.shape}\")\n",
    "pickle.dump(preprocessor, open('../preprocessor.pkl', 'wb'))\n",
    "print(\"preprocessor.pkl saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rest = KMeans(n_clusters=12, random_state=42, n_init=10)\n",
    "restaurants_df['restaurant_cluster'] = kmeans_rest.fit_predict(X_final)\n",
    "pickle.dump(kmeans_rest, open('../kmeans_restaurants.pkl', 'wb'))\n",
    "\n",
    "# city level clustering\n",
    "city_agg = restaurants_df.groupby('city').agg({\n",
    "    'stars': 'mean',\n",
    "    'price_range': 'mean',\n",
    "    'cluster_density': 'mean',\n",
    "    'checkin_count': 'mean',\n",
    "    'success_score': 'mean',\n",
    "    'review_count': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "city_numeric_correct_order = pd.DataFrame({\n",
    "    'latitude': 0,                          # dummy\n",
    "    'longitude': 0,                         # dummy\n",
    "    'price_range': city_agg['price_range'],\n",
    "    'checkin_count': city_agg['checkin_count'],\n",
    "    'cluster_density': city_agg['cluster_density']\n",
    "})\n",
    "\n",
    "scaler = preprocessor.named_transformers_['num']\n",
    "city_scaled = scaler.transform(city_numeric_correct_order)\n",
    "\n",
    "kmeans_city = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
    "city_agg['city_cluster'] = kmeans_city.fit_predict(city_scaled)\n",
    "\n",
    "restaurants_df = restaurants_df.merge(city_agg[['city', 'city_cluster']], on='city', how='left')\n",
    "pickle.dump(kmeans_city, open('../kmeans_cities.pkl', 'wb'))\n",
    "\n",
    "fig = px.bar(\n",
    "    x=range(12),\n",
    "    y=restaurants_df['restaurant_cluster'].value_counts().sort_index(),\n",
    "    title=\"Restaurant Cluster Distributions\",\n",
    "    labels={'x': 'Cluster ID', 'y': 'Number of Restaurants'},\n",
    "    color=range(12),\n",
    "    color_continuous_scale=\"Viridis\"\n",
    ")\n",
    "fig.update_layout(showlegend=False, height=500)\n",
    "fig.show()\n",
    "\n",
    "# print(city_agg[['city', 'city_cluster']].sort_values('city_cluster').head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"discovered city market types:\\n\")\n",
    "print(city_agg.groupby('city_cluster').agg({\n",
    "    'city': 'count',\n",
    "    'review_count': 'mean',\n",
    "    'cluster_density': 'mean',\n",
    "    'success_score': 'mean'\n",
    "}).round(2))\n",
    "\n",
    "print(\"\\nTop cities in each cluster:\")\n",
    "for i in range(6):\n",
    "    top = city_agg[city_agg['city_cluster'] == i][['city', 'review_count']].sort_values('review_count', ascending=False).head(5)\n",
    "    print(f\"\\nCluster {i} (n={len(city_agg[city_agg['city_cluster']==i])} cities):\")\n",
    "    print(top.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The city level KMeans clustering reveals that there are 6 distinct restaurant market types.\n",
    "\n",
    "**Cluster 0:** represents smaller towns with lower density and more modest success scores\n",
    "\n",
    "**Clusters 1-5:** major metropolitan areas\n",
    "\n",
    "This shows that location type is a strong feature to predict restaurant success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "start = time.time()\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=18, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "results.append({\n",
    "    'Model': 'Decision Tree',\n",
    "    'R2': round(r2_score(y_test, dt_pred), 4),\n",
    "    'Mean absolute error': round(mean_absolute_error(y_test, dt_pred), 2),\n",
    "    'Time in seconds': round(time.time() - start, 1)\n",
    "})\n",
    "\n",
    "print(\"Gradient Boosting\")\n",
    "start = time.time()\n",
    "gb = GradientBoostingRegressor(n_estimators=300, max_depth=6, learning_rate=0.1, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_test)\n",
    "results.append({\n",
    "    'Model': 'Gradient Boosting',\n",
    "    'R2': round(r2_score(y_test, gb_pred), 4),\n",
    "    'Mean absolute error': round(mean_absolute_error(y_test, gb_pred), 2),\n",
    "    'Time in seconds': round(time.time() - start, 1)\n",
    "})\n",
    "\n",
    "print(\"Random Forest\")\n",
    "start = time.time()\n",
    "rf = RandomForestRegressor(n_estimators=400, max_depth=22, n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "results.append({\n",
    "    'Model': 'Random Forest (Deployed)',\n",
    "    'R2': round(r2_score(y_test, rf_pred), 4),\n",
    "    'Mean absolute error': round(mean_absolute_error(y_test, rf_pred), 2),\n",
    "    'Time in seconds': round(time.time() - start, 1)\n",
    "})\n",
    "\n",
    "comparison = pd.DataFrame(results).sort_values('R2', ascending=False)\n",
    "print(\"*\" * 20 + \" FINAL MODEL COMPARISON \" + \"*\" * 20)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# save final model\n",
    "pickle.dump(rf, open('../final_model.pkl', 'wb'))\n",
    "pickle.dump(preprocessor, open('../preprocessor.pkl', 'wb'))\n",
    "print(\"\\nfinal_model.pkl + preprocessor.pkl saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Static generation of the plots were needed, as Plotly figures were not appearing in the HTML and notebook file in Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "os.makedirs(\"static_plots\", exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "top_cities = restaurants_df['city'].value_counts().head(10)\n",
    "axes[0,0].barh(top_cities.index[::-1], top_cities.values[::-1], color='crimson')\n",
    "axes[0,0].set_title(\"Top 10 Cities by Restaurant Count\", fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel(\"Number of Restaurants\")\n",
    "\n",
    "axes[0,1].hist(restaurants_df['success_score'], bins=50, color='seagreen', alpha=0.8)\n",
    "axes[0,1].set_title(\"Success Score Distribution (Percentile within City)\", fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_xlabel(\"Success Score\")\n",
    "axes[0,1].axvline(30, color='red', linestyle='--', label=\"Bottom 30% (Underserved)\")\n",
    "axes[0,1].legend()\n",
    "\n",
    "sns.scatterplot(data=restaurants_df.sample(5000), x='cluster_density', y='success_score',\n",
    "                hue='price_range', palette='viridis', ax=axes[1,0], alpha=0.6)\n",
    "axes[1,0].set_title(\"Market Hotspot Density vs Success Score\", fontsize=14, fontweight='bold')\n",
    "\n",
    "cluster_counts = restaurants_df['restaurant_cluster'].value_counts().sort_index()\n",
    "axes[1,1].bar(cluster_counts.index, cluster_counts.values, color='skyblue', edgecolor='black')\n",
    "axes[1,1].set_title(\"12 Restaurant Market Segments (KMeans)\", fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel(\"Cluster ID\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"static_plots/all_key_insights.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
